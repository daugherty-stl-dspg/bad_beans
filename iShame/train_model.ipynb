{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the training and test data set that we created earlier. We also set a flag for whether the expected values are one-hot encoded classes or the range of coffee pot fullness (0.0 - 1.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_regression = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_save_path = '../../data/coffee_video/label_data/v3'\n",
    "class_data_set_name='coffee_class_data_v3.h5'\n",
    "regression_data_set_name='coffee_regression_data_v3.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Training Data Shape: (1848, 540, 960, 3)\nTraining Labels Shape: (1848, 6)\nTest Data Shape: (462, 540, 960, 3)\nTest Labels Shape: (462, 6)\n"
    }
   ],
   "source": [
    "def get_data(filepath):\n",
    "    data_file = h5py.File(filepath,'r')\n",
    "\n",
    "    train_data = data_file['train_data'][:]\n",
    "    train_labels = data_file['train_labels'][:]\n",
    "\n",
    "    test_data = data_file['test_data'][:]\n",
    "    test_labels = data_file['test_labels'][:]\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "if(is_regression):\n",
    "    filepath=os.path.join(data_save_path,regression_data_set_name)\n",
    "else:\n",
    "    filepath=os.path.join(data_save_path,class_data_set_name)\n",
    "\n",
    "train_data, train_labels, test_data, test_labels  = get_data(filepath)\n",
    "\n",
    "print('Training Data Shape:',train_data.shape)\n",
    "print('Training Labels Shape:',train_labels.shape)\n",
    "\n",
    "print('Test Data Shape:',test_data.shape)\n",
    "print('Test Labels Shape:',test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define our model. Using keras, we define the first layer of the model based on the image dimensions, which would be (width, height, channels). After this, we add a series of 2D convolutional layers which consists of a covolution operation, a non-linear activation, and then a max pooling layer. Hyperparameters for the number of filters, filter size, etc. are defined in a list for each layer. The convolutional layers act as spatial feature detectors which learn different feature representations which will be used by the next steps.\n",
    "\n",
    "Then, the output is flattened, and we add a number of dense fully connected layers that are also defined as a list. Each layer consists of the numbber of hidden units,  \n",
    "\n",
    "Finally, we add the output layer. If we are using this as a multi-class problem (e.g. varying amounts of coffee pot fullness are bucketed and each is treated as a separate class), then our final output layers consists of a neuron for each class, with softmax activation. Otherwise, in the regression case (where we treat the output as a continuous variable), we add a dense layer with a single node which would output values betwen the range of 0-1. Mean squared error is then used as the loss function to train the network.\n",
    "\n",
    "The model is defined in a JSON file. First, let's load the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "conv_layers [{'num_filters': 16, 'filter_size': '(5,5)', 'stride_size': '(2,2)', 'activation': 'relu', 'pool_size': '(2,2)'}, {'num_filters': 32, 'filter_size': '(5,5)', 'stride_size': '(2,2)', 'activation': 'relu', 'pool_size': '(2,2)'}, {'num_filters': 64, 'filter_size': '(5,5)', 'stride_size': '(2,2)', 'activation': 'relu', 'pool_size': '(2,2)'}]\nconn_layers [{'hidden_units': 20, 'activation': 'relu', 'dropout': 0.5}, {'hidden_units': 10, 'activation': 'relu'}]\n"
    }
   ],
   "source": [
    "import build_ishame_model\n",
    "\n",
    "iShame_model_config = 'iShameModel.json'\n",
    "conv_layers, conn_layers = build_ishame_model.load_model_def(iShame_model_config)\n",
    "print(\"conv_layers\",conv_layers)\n",
    "print(\"conn_layers\",conn_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define some additional hyperparameters for our model, based on whether we are treating this as a classification or regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = train_data.shape[1:]\n",
    "if(is_regression):\n",
    "    num_classes=None\n",
    "else:\n",
    "    num_classes=train_labels.shape[-1]\n",
    "batch_size=32\n",
    "epochs=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to build our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_ishame_model.build_model(input_shape=input_shape, \\\n",
    "                                        conv_layers=conv_layers, \\\n",
    "                                        conn_layers=conn_layers, \\\n",
    "                                        output_classes=num_classes)\n",
    "build_ishame_model.compile_model(model, is_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the different layers. We can the convolutional and fully connected layers defined in the config have been successfully created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv_0 (Conv2D)              (None, 268, 478, 16)      1216      \n_________________________________________________________________\nmax_pool_0 (MaxPooling2D)    (None, 134, 239, 16)      0         \n_________________________________________________________________\nconv_1 (Conv2D)              (None, 65, 118, 32)       12832     \n_________________________________________________________________\nmax_pool_1 (MaxPooling2D)    (None, 32, 59, 32)        0         \n_________________________________________________________________\nconv_2 (Conv2D)              (None, 14, 28, 64)        51264     \n_________________________________________________________________\nmax_pool_2 (MaxPooling2D)    (None, 7, 14, 64)         0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 6272)              0         \n_________________________________________________________________\nfully_connected_0 (Dense)    (None, 20)                125460    \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 20)                0         \n_________________________________________________________________\nfully_connected_1 (Dense)    (None, 10)                210       \n_________________________________________________________________\nclass_output_6 (Dense)       (None, 6)                 66        \n=================================================================\nTotal params: 191,048\nTrainable params: 191,048\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "OSError",
     "evalue": "`pydot` failed to call GraphViz.Please install GraphViz (https://www.graphviz.org/) and ensure that its executables are in the $PATH.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m~/.virtualenvs/cv/lib/python3.7/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# to check the pydot/graphviz installation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cv/lib/python3.7/site-packages/pydot.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, prog, format, encoding)\u001b[0m\n\u001b[1;32m   1914\u001b[0m                 \u001b[0marguments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1915\u001b[0;31m                 \u001b[0mworking_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtmp_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1916\u001b[0m             )\n",
      "\u001b[0;32m~/.virtualenvs/cv/lib/python3.7/site-packages/pydot.py\u001b[0m in \u001b[0;36mcall_graphviz\u001b[0;34m(program, arguments, working_dir, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m     )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    774\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    776\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1452\u001b[0m                             \u001b[0merrpipe_read\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrpipe_write\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1453\u001b[0;31m                             restore_signals, start_new_session, preexec_fn)\n\u001b[0m\u001b[1;32m   1454\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_child_created\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 22] Invalid argument",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-52d09bb01d1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/cv/lib/python3.7/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[0;34m(model, to_file, show_shapes, show_layer_names, rankdir, expand_nested, dpi)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \"\"\"\n\u001b[1;32m    239\u001b[0m     dot = model_to_dot(model, show_shapes, show_layer_names, rankdir,\n\u001b[0;32m--> 240\u001b[0;31m                        expand_nested, dpi)\n\u001b[0m\u001b[1;32m    241\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mextension\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cv/lib/python3.7/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[0;34m(model, show_shapes, show_layer_names, rankdir, expand_nested, dpi, subgraph)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0m_check_pydot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msubgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dashed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cv/lib/python3.7/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         raise OSError(\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0;34m'`pydot` failed to call GraphViz.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0;34m'Please install GraphViz (https://www.graphviz.org/) '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             'and ensure that its executables are in the $PATH.')\n",
      "\u001b[0;31mOSError\u001b[0m: `pydot` failed to call GraphViz.Please install GraphViz (https://www.graphviz.org/) and ensure that its executables are in the $PATH."
     ]
    }
   ],
   "source": [
    "plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we train our model with the given training data and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/10\n1848/1848 [==============================] - 596s 323ms/step - loss: 1.6420 - categorical_accuracy: 0.2744\nEpoch 2/10\n1848/1848 [==============================] - 532s 288ms/step - loss: 1.3636 - categorical_accuracy: 0.3409\nEpoch 3/10\n1848/1848 [==============================] - 375s 203ms/step - loss: 1.2751 - categorical_accuracy: 0.3772\nEpoch 4/10\n1848/1848 [==============================] - 360s 195ms/step - loss: 1.1872 - categorical_accuracy: 0.4616\nEpoch 5/10\n1848/1848 [==============================] - 365s 198ms/step - loss: 1.1468 - categorical_accuracy: 0.4816\nEpoch 6/10\n1848/1848 [==============================] - 367s 199ms/step - loss: 1.1024 - categorical_accuracy: 0.4957\nEpoch 7/10\n1848/1848 [==============================] - 390s 211ms/step - loss: 1.0387 - categorical_accuracy: 0.5281\nEpoch 8/10\n1848/1848 [==============================] - 397s 215ms/step - loss: 1.0109 - categorical_accuracy: 0.5455\nEpoch 9/10\n1848/1848 [==============================] - 421s 228ms/step - loss: 0.9992 - categorical_accuracy: 0.5547\nEpoch 10/10\n1848/1848 [==============================] - 462s 250ms/step - loss: 0.9906 - categorical_accuracy: 0.5687\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<keras.callbacks.callbacks.History at 0x7fe12a022dd8>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "model.fit(train_data, train_labels, epochs = epochs, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's verify our model performance by checking our performance on our holdout set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "462/462 [==============================] - 68s 147ms/step\nLoss: 0.643123876093786\nMetric: 0.8333333134651184\n"
    }
   ],
   "source": [
    "results = model.evaluate(test_data, test_labels)\n",
    "print(\"Loss:\",results[0])\n",
    "print(\"Metric:\",results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = '../../data/coffee_video/model'\n",
    "\n",
    "if(is_regression):\n",
    "    model_weights_file='coffee_regression_v3_sequential_weights.h5'\n",
    "else:\n",
    "    model_weights_file='coffee_class_v3_sequential_weights.h5'\n",
    "\n",
    "model.save_weights(os.path.join(model_save_path,model_weights_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "opencv",
   "display_name": "opencv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}