{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "from keras.layers import Input, Dense, Activation, MaxPooling2D, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.utils import plot_model\n",
    "from keras.metrics import MeanSquaredError, CategoricalAccuracy\n",
    "\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the training and test data set that we created earlier. We also set a flag for whether the expected values are one-hot encoded classes or the range of coffee pot fullness (0.0 - 1.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_labels = True\n",
    "data_save_path = '../../img/video/label_data/v2'\n",
    "data_set_name='coffee_regression_label_data_v2.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Training Data Shape: (1848, 540, 960, 3)\nTest Data Shape: (462, 540, 960, 3)\n"
    }
   ],
   "source": [
    "def get_data(filepath):\n",
    "    data_file = h5py.File(filepath,'r')\n",
    "\n",
    "    train_data = data_file['train_data'][:]\n",
    "    train_labels = data_file['train_labels'][:]\n",
    "\n",
    "    test_data = data_file['test_data'][:]\n",
    "    test_labels = data_file['test_labels'][:]\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "filepath=os.path.join(data_save_path,data_set_name)\n",
    "train_data, train_labels, test_data, test_labels  = get_data(filepath)\n",
    "print('Training Data Shape:',train_data.shape)\n",
    "print('Test Data Shape:',test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define our model. Using keras, we define the first layer of the model based on the image dimensions, which would be (width, height, channels). After this, we add a series of 2D convolutional layers which consists of a covolution operation, a non-linear activation, and then a max pooling layer. Hyperparameters for the number of filters, filter size, etc. are defined in a list for each layer. The convolutional layers act as spatial feature detectors which learn different feature representations which will be used by the next steps.\n",
    "\n",
    "Then, the output is flattened, and we add a number of dense fully connected layers that are also defined as a list. Each layer consists of the numbber of hidden units,  \n",
    "\n",
    "Finally, we add the output layer. If we are using this as a multi-class problem (e.g. varying amounts of coffee pot fullness are bucketed and each is treated as a separate class), then our final output layers consists of a neuron for each class, with softmax activation. Otherwise, in the regression case (where we treat the output as a continuous variable), we add a dense layer with a single node which would output values betwen the range of 0-1. Mean squared error is then used as the loss function to train the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iShameModel(input_shape, conv_layers, conn_layers, output_classes=None):\n",
    "\n",
    "    # input layer\n",
    "    X_input = Input(input_shape)\n",
    "    X = X_input\n",
    "\n",
    "    # convlutional layers\n",
    "    # CONV2D -> RELU -> MAXPOOL\n",
    "    # number of filters, filter_size, and strides retrieved from input conv array\n",
    "\n",
    "    for idx, layer in enumerate(conv_layers):        \n",
    "        X = Conv2D(layer[\"num_filters\"], layer[\"filter_size\"], strides = layer[\"filter_size\"], \\\n",
    "                    name = 'conv_'+str(idx), kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "        #X = BatchNormalization(axis = 3, name = 'bn_conv1')(X)\n",
    "        X = Activation(layer[\"activation\"])(X)\n",
    "        X = MaxPooling2D(layer[\"filter_size\"], strides=layer[\"stride_size\"], name=\"max_pool_\"+str(idx))(X)\n",
    "\n",
    "    # add fully connected layers\n",
    "    X = Flatten()(X)\n",
    "\n",
    "    for idx, layer in enumerate(conn_layers):\n",
    "        X = Dense(layer[\"hidden_units\"], activation=layer[\"activation\"], name=\"fully_connected_\"+str(idx))(X)\n",
    "        if \"dropout\" in layer:\n",
    "            X = Dropout(layer[\"dropout\"])(X)\n",
    "\n",
    "    # if classification, add softmax output layer\n",
    "    # otherwise, add singular output layer with one unit with NO activation\n",
    "    if output_classes is not None:\n",
    "        X = Dense(output_classes, activation='softmax', name='class_output_'+str(output_classes), kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    else:\n",
    "        X = Dense(1, name = 'regression_output')(X)\n",
    "\n",
    "    model = Model(inputs=X_input, outputs=X, name='iShame')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define the layers for our CNN, as explained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layers = [{\"num_filters\": 16,\n",
    "                \"filter_size\": (3,3),\n",
    "                \"stride_size\": (1,1),\n",
    "                \"activation\": \"relu\",\n",
    "                \"pool_size\": (2,2)\n",
    "                },\n",
    "                {\"num_filters\": 32,\n",
    "                \"filter_size\": (3,3),\n",
    "                \"stride_size\": (1,1),\n",
    "                \"activation\": \"relu\",\n",
    "                \"pool_size\": (2,2)\n",
    "                },\n",
    "                {\"num_filters\": 64,\n",
    "                \"filter_size\": (3,3),\n",
    "                \"stride_size\": (1,1),\n",
    "                \"activation\": \"relu\",\n",
    "                \"pool_size\": (2,2)\n",
    "                }]\n",
    "conn_layers = [{\"hidden_units\": 500,\n",
    "                \"activation\": \"relu\",\n",
    "                \"dropout\": 0.5\n",
    "                },\n",
    "                {\"hidden_units\": 100,\n",
    "                \"activation\": \"relu\"\n",
    "                },\n",
    "                {\"hidden_units\": 20,\n",
    "                    \"activation\": \"relu\"\n",
    "                }]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define some additional hyperparameters for our model, based on whether we are treating this as a classification or regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(regression_labels):\n",
    "    num_classes=None\n",
    "    loss=\"mean_squared_error\"\n",
    "    metrics=[MeanSquaredError()]\n",
    "else:\n",
    "    num_classes=train_labels.shape[-1]\n",
    "    loss=\"categorical_crossentropy\"\n",
    "    metrics=[CategoricalAccuracy()]\n",
    "batch_size=32\n",
    "epochs=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to build our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = iShameModel(input_shape=train_data.shape[1:],conv_layers=conv_layers,conn_layers=conn_layers,output_classes=num_classes)\n",
    "model.compile(optimizer='adam', loss=loss, metrics=metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the different layers. We can the convolutional and fully connected layers defined in the config have been successfully created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"iShame\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         (None, 540, 960, 3)       0         \n_________________________________________________________________\nconv_0 (Conv2D)              (None, 180, 320, 16)      448       \n_________________________________________________________________\nactivation_1 (Activation)    (None, 180, 320, 16)      0         \n_________________________________________________________________\nmax_pool_0 (MaxPooling2D)    (None, 178, 318, 16)      0         \n_________________________________________________________________\nconv_1 (Conv2D)              (None, 59, 106, 32)       4640      \n_________________________________________________________________\nactivation_2 (Activation)    (None, 59, 106, 32)       0         \n_________________________________________________________________\nmax_pool_1 (MaxPooling2D)    (None, 57, 104, 32)       0         \n_________________________________________________________________\nconv_2 (Conv2D)              (None, 19, 34, 64)        18496     \n_________________________________________________________________\nactivation_3 (Activation)    (None, 19, 34, 64)        0         \n_________________________________________________________________\nmax_pool_2 (MaxPooling2D)    (None, 17, 32, 64)        0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 34816)             0         \n_________________________________________________________________\nfully_connected_0 (Dense)    (None, 500)               17408500  \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 500)               0         \n_________________________________________________________________\nfully_connected_1 (Dense)    (None, 100)               50100     \n_________________________________________________________________\nfully_connected_2 (Dense)    (None, 20)                2020      \n_________________________________________________________________\nregression_output (Dense)    (None, 1)                 21        \n=================================================================\nTotal params: 17,484,225\nTrainable params: 17,484,225\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "OSError",
     "evalue": "`pydot` failed to call GraphViz.Please install GraphViz (https://www.graphviz.org/) and ensure that its executables are in the $PATH.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m~/.virtualenvs/cv/lib/python3.7/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# to check the pydot/graphviz installation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cv/lib/python3.7/site-packages/pydot.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, prog, format, encoding)\u001b[0m\n\u001b[1;32m   1914\u001b[0m                 \u001b[0marguments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1915\u001b[0;31m                 \u001b[0mworking_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtmp_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1916\u001b[0m             )\n",
      "\u001b[0;32m~/.virtualenvs/cv/lib/python3.7/site-packages/pydot.py\u001b[0m in \u001b[0;36mcall_graphviz\u001b[0;34m(program, arguments, working_dir, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m     )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    774\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    776\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1452\u001b[0m                             \u001b[0merrpipe_read\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrpipe_write\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1453\u001b[0;31m                             restore_signals, start_new_session, preexec_fn)\n\u001b[0m\u001b[1;32m   1454\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_child_created\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 22] Invalid argument",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-52d09bb01d1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/cv/lib/python3.7/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[0;34m(model, to_file, show_shapes, show_layer_names, rankdir, expand_nested, dpi)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \"\"\"\n\u001b[1;32m    239\u001b[0m     dot = model_to_dot(model, show_shapes, show_layer_names, rankdir,\n\u001b[0;32m--> 240\u001b[0;31m                        expand_nested, dpi)\n\u001b[0m\u001b[1;32m    241\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mextension\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cv/lib/python3.7/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[0;34m(model, show_shapes, show_layer_names, rankdir, expand_nested, dpi, subgraph)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0m_check_pydot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msubgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dashed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cv/lib/python3.7/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         raise OSError(\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0;34m'`pydot` failed to call GraphViz.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0;34m'Please install GraphViz (https://www.graphviz.org/) '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             'and ensure that its executables are in the $PATH.')\n",
      "\u001b[0;31mOSError\u001b[0m: `pydot` failed to call GraphViz.Please install GraphViz (https://www.graphviz.org/) and ensure that its executables are in the $PATH."
     ]
    }
   ],
   "source": [
    "plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we train our model with the given training data and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/10\n1848/1848 [==============================] - 364s 197ms/step - loss: 0.4577 - mean_squared_error: 0.4577\nEpoch 2/10\n1848/1848 [==============================] - 392s 212ms/step - loss: 0.0190 - mean_squared_error: 0.0190\nEpoch 3/10\n1848/1848 [==============================] - 373s 202ms/step - loss: 0.0110 - mean_squared_error: 0.0110\nEpoch 4/10\n1848/1848 [==============================] - 382s 207ms/step - loss: 0.0073 - mean_squared_error: 0.0073\nEpoch 5/10\n1848/1848 [==============================] - 398s 216ms/step - loss: 0.0060 - mean_squared_error: 0.0060\nEpoch 6/10\n1848/1848 [==============================] - 404s 219ms/step - loss: 0.0048 - mean_squared_error: 0.0048\nEpoch 7/10\n1848/1848 [==============================] - 407s 220ms/step - loss: 0.0048 - mean_squared_error: 0.0048\nEpoch 8/10\n1848/1848 [==============================] - 393s 213ms/step - loss: 0.0042 - mean_squared_error: 0.0042\nEpoch 9/10\n1848/1848 [==============================] - 391s 212ms/step - loss: 0.0034 - mean_squared_error: 0.0034\nEpoch 10/10\n1848/1848 [==============================] - 381s 206ms/step - loss: 0.0033 - mean_squared_error: 0.0033\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<keras.callbacks.callbacks.History at 0x7face7ae3d68>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "model.fit(train_data, train_labels, epochs = epochs, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's verify our model performance by checking our performance on our holdout set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "462/462 [==============================] - 50s 109ms/step\nLoss: 0.011002382086146446\nMetric: 0.011002382263541222\n"
    }
   ],
   "source": [
    "results = model.evaluate(test_data, test_labels)\n",
    "print(\"Loss:\",results[0])\n",
    "print(\"Metric:\",results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = '../../img/video/model'\n",
    "model_name='coffee_regression_label_data_v2.model'\n",
    "model.save(os.path.join(model_save_path,model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "opencv",
   "display_name": "opencv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}